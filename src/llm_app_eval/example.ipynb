{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import sys\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "from llm_app import BaseApp, InputFormat, OutputFormat, GptBaseApp\n",
    "from evaluator import TestCase, EvalProperty, PropertyResult, Evaluator\n",
    "from eval_properties import evaluate_property_with_llm, cosine_similarity, get_embedding\n",
    "\n",
    "import openai\n",
    "import instructor\n",
    "\n",
    "openai.api_key_path = \"../../openai_key\"\n",
    "instructor.patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some tests.\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        test_id=uuid.uuid4().hex,\n",
    "        test_input={\"question\": \"Why should a victim go to the doctor after a Heimlich manoeuvre?\"},\n",
    "        reference_output={\"answer\": \"Because the Heimlich manoeuvre may have caused internal bleeding.\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        test_id=uuid.uuid4().hex,\n",
    "        test_input={\"question\": \"What are the four steps of first aid?\"},\n",
    "        reference_output={\"answer\": \"1. Ensure safety, 2. Assess the victim's condition, 3. Notify emergency services if necessary, 4. Provide further first aid.\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        test_id=uuid.uuid4().hex,\n",
    "        test_input={\"question\": \"What should you do if the victim is not breathing?\"},\n",
    "        reference_output={\"answer\": \"Call the emergencies and start CPR.\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define properties.\n",
    "def factually_consistent(test_case: TestCase, llm_app_result: OutputFormat) -> PropertyResult:\n",
    "    return evaluate_property_with_llm(\n",
    "        model=\"gpt-4\",\n",
    "        system_message=\"Evaluate the answer. The answer should be factually consistent with the reference answer. If not, explain why.\",\n",
    "        user_message=f\"Answer: {llm_app_result.answer}\\nReference Answer: {test_case.reference_output.answer}\",\n",
    "    )\n",
    "\n",
    "def output_similarity(test_case: TestCase, llm_app_result: OutputFormat) -> PropertyResult:\n",
    "    app_output_emb = get_embedding(llm_app_result.answer)\n",
    "    reference_emb = get_embedding(test_case.reference_output.answer)\n",
    "    return PropertyResult(feedback=\"\", score=cosine_similarity(app_output_emb, reference_emb))\n",
    "\n",
    "\n",
    "def output_verbosity(test_case: TestCase, llm_app_result: OutputFormat) -> PropertyResult:\n",
    "    return PropertyResult(feedback=\"\", score=len(llm_app_result.answer) / len(test_case.reference_output.answer))\n",
    "\n",
    "properties = [\n",
    "    EvalProperty(\n",
    "        property_name=\"FactuallyConsistent\",\n",
    "        description=\"The answer is factually consistent with the reference answer.\",\n",
    "        eval_func=factually_consistent,\n",
    "    ),\n",
    "    EvalProperty(\n",
    "        property_name=\"CosineSimilarity\",\n",
    "        description=\"The answer is similar to the reference answer.\",\n",
    "        eval_func=output_similarity,\n",
    "    ),\n",
    "    EvalProperty(\n",
    "        property_name=\"Verbosity\",\n",
    "        description=\"The answer is not too verbose.\",\n",
    "        eval_func=output_verbosity,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM app versions.\n",
    "llm_apps = [\n",
    "    GptBaseApp({\n",
    "        \"gpt_version\": \"gpt-3.5-turbo-0613\",\n",
    "        \"system_prompt\": \"Answer the question.\"\n",
    "    }),\n",
    "    GptBaseApp({\n",
    "        \"gpt_version\": \"gpt-3.5-turbo-0613\",\n",
    "        \"system_prompt\": \"You are a first-aid expert. Answer the question. Be accurate and concise.\"\n",
    "    }),\n",
    "    GptBaseApp({\n",
    "        \"gpt_version\": \"gpt-4\",\n",
    "        \"system_prompt\": \"You are a first-aid expert. Answer the question. Be accurate and concise.\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test cases: 100%|██████████| 3/3 [00:23<00:00,  7.75s/test case]\n",
      "Evaluating test cases: 100%|██████████| 3/3 [00:20<00:00,  6.79s/test case]\n",
      "Evaluating test cases: 100%|██████████| 3/3 [00:29<00:00,  9.91s/test case]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the LLM apps on the test set by using the properties.\n",
    "ev = Evaluator(test_set=test_cases, properties=properties, results_dir=\"data/eval_results\")\n",
    "exp_name = input(\"Experiment name: \")\n",
    "results_df = ev.evaluate(llm_apps, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt_version</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>CosineSimilarity.score</th>\n",
       "      <th>FactuallyConsistent.score</th>\n",
       "      <th>Verbosity.score</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>Answer the question.</td>\n",
       "      <td>0.904431</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>8.370790</td>\n",
       "      <td>2.814149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>You are a first-aid expert. Answer the questio...</td>\n",
       "      <td>0.903006</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.031130</td>\n",
       "      <td>1.684336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4</td>\n",
       "      <td>You are a first-aid expert. Answer the questio...</td>\n",
       "      <td>0.907844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.116838</td>\n",
       "      <td>5.495382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gpt_version                                      system_prompt  \\\n",
       "0  gpt-3.5-turbo-0613                               Answer the question.   \n",
       "0  gpt-3.5-turbo-0613  You are a first-aid expert. Answer the questio...   \n",
       "0               gpt-4  You are a first-aid expert. Answer the questio...   \n",
       "\n",
       "   CosineSimilarity.score  FactuallyConsistent.score  Verbosity.score  \\\n",
       "0                0.904431                   0.666667         8.370790   \n",
       "0                0.903006                   0.666667         2.031130   \n",
       "0                0.907844                   1.000000         5.116838   \n",
       "\n",
       "    latency  \n",
       "0  2.814149  \n",
       "0  1.684336  \n",
       "0  5.495382  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Mlflow dashboard.\n",
    "!mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-app-eval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
